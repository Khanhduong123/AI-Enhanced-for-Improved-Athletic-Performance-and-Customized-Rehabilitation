{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CÁC PHƯƠNG PHÁP AUGEMENTATION\n",
    "\n",
    "1. JITTERING (thêm nhiễu vào các keypoints)\n",
    "- Thêm nhiễu guassian (norm distribution) vào các keypoints\n",
    "- giúp mô hình không bị phụ thuộc vào vị trí chính xác của keypoints\n",
    "- giúp mô hình học được các biến thể của keypoints\n",
    "- công thức: skeleton(aug) = skeleton + N(0, sigma^2) với N(0, sigma^2) là phân phối Guassian có trung bình 0 và độ lệch chuẩn sigma\n",
    "- Dùng khi nào? Khi muốn mô hình học được sự biến đổi nhỏ trong tọa độ keypoints, khi dữ liệu quá sạch và dễ bị overfitting\n",
    "- VD: ban đầu: (0.5, 0.3, 0.2) --> jittering (với noise nhỏ): (0.502, 0.295, 0.205)\n",
    "2. Scaling (phóng to/ thu nhỏ toàn bộ skeleton)\n",
    "- Phóng to hoặc thu nhỏ keypoints theo một hệ số ngẫu nhiên.\n",
    "- Giúp mô hình không bị phụ thuộc vào kích thước người tập yoga.\n",
    "- Công thức: skeleton(aug) = skeleton * scale_factor (trong này set là trong khoảng 0.9 và 1.1)\n",
    "- Dùng khi nào? Khi muốn mô hình học được sự biến đổi về kích thước của người tập yoga (cao thấp, mập ốm,...) --> không bị ảnh hưởng bởi kích thước tuyệt đối\n",
    "- VD: ban đầu: (0.5, 0.3, 0.2) --> scaling (scale_factor = 1.1): (0.55, 0.33, 0.22)\n",
    "3. Rotation (Xoay toàn bộ skeleton quanh trung tâm)\n",
    "- Xoay khung xương quanh trung tâm theo góc ngẫu nhiên (-15 đến 15 độ)\n",
    "- Giúp mô hình hiểu được các tư thế nhìn từ nhiều góc độ khác nhau.\n",
    "- Công thức: \n",
    "    * R = [[cos(theta), -sin(theta)], [sin(theta), cos(theta)]] với theta là góc xoay\n",
    "    * skeleton(aug) = R * (skeleton - center)*R + center\n",
    "- Dùng khi nào? Khi muốn mô hình học được sự biến đổi về góc nhìn của người tập yoga, phòng khi data chỉ thu được từ một góc độ cố định\n",
    "- VD: Nếu điểm tay ban đầu: (0.5, 0.3) --> xoay 10 độ (0.48, 0.32)\n",
    "4. Horizontal Flip (Lật keypoints trái/phải)\n",
    "- Lật toàn bộ tư thế theo trục X\n",
    "- Giúp mô hình hiểu cả tư thế từ cả hai phía trái phải của cùng một động tác\n",
    "- công thức: skeleton(aug) = [:, :, 0] = 1 - skeleton[:, :, 0]\n",
    "- Dùng khi nào? Khi muốn mô hình học được sự biến đổi về trái phải của người tập yoga, khi dữ liệu chỉ có một phiên bản của động tác\n",
    "- VD: ban đầu (0.2, 0.5) --> flip (0.8, 0.5)\n",
    "5. Temporal Warping (Tăng/Giảm tốc độ chuyển động)\n",
    "- Tăng hoặc giảm tốc độ của video mà không làm mất động tác.\n",
    "- Giúp mô hình hiểu được động tác thực hiện nhanh/chậm khác nhau.\n",
    "- công thức: \n",
    "    * num_frames = T * warp_factor\n",
    "    * skeleton(aug) = skeleton[indices]\n",
    "    * với warp_factor (0.8, 1.2) là hệ số tốc độ, indices là các chỉ số của frame sau khi tăng/giảm tốc độ\n",
    "- Dùng khi nào? Khi động tác có thể thực hiện với tốc độ khác nhau, khi video có frame_rate không đồng đều\n",
    "- VD: ban đầu: 10 frames --> giảm tốc độ (warp_factor = 1.2): 12 frames\n",
    "6. Time Masking (Ẩn một số frame)\n",
    "- Loại bỏ một số frame ngẫu nhiên, giả lập video bị mất frame hoặc hành động không liên tục.\n",
    "- Giúp mô hình học cách đối phó với dữ liệu không liên tục.\n",
    "- Dùng khi nào? Khi video có tốc độ ghi hình không ổn định, khi muốn mô hình không bị phụ thuộc vào toàn bộ chuỗi frame.\n",
    "- VD: ban đầu: [F1, F2, F3, F4, F5] --> Sau masking: [F1, 0, F3, 0, F5]\n",
    "7. Frame Interpolation (Nội suy khung hình)\n",
    "- Tạo thêm frame nội suy giữa các frame thực tế, giúp mô hình học mượt hơn.\n",
    "- Giả lập dữ liệu có tốc độ quay cao hơn.\n",
    "- Dùng khi nào? Khi video bị ít frame, cần tạo thêm frame để training, khi muốn mô hình học được chuyển động mượt mà hơn.\n",
    "- VD: ban đầu: [F1, F2, F3] --> Interpolation: [F1, F1.5, F2, F2.5, F3]\n",
    "\n",
    "==================================KẾT LUẬN================================\n",
    "- Nếu muốn tăng độ đa dạng, thử Jittering + Rotation + Scaling.\n",
    "- Nếu dữ liệu bị thiếu keypoints, thử Keypoint Masking + Time Masking.\n",
    "- Nếu dữ liệu có tốc độ khác nhau, thử Temporal Warping + Frame Interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUGMENT THẾ NÀO CHO ĐÚNG?\n",
    "1. Augmentation phải giữ nguyên bản chất dữ liệu\n",
    "- Augmentation không được làm mất đi cấu trúc động tác.\n",
    "    * Horizontal Flip có thể sai nếu động tác chỉ có một chiều nhất định. (Sai)\n",
    "    * Rotation quá mạnh (>30°) có thể làm sai tư thế. (Sai)\n",
    "    * Jittering chỉ nên thêm nhiễu nhỏ (±0.01) để tránh làm thay đổi vị trí động tác. (Đúng)\n",
    "    * Rotation chỉ nên xoay trong khoảng ±15° để giữ đúng cấu trúc khung xương. (Đúng)\n",
    "2. Augmentation phải đa dạng nhưng không gây nhiễu quá mức\n",
    "- Không nên áp dụng quá nhiều augmentation cùng lúc, vì sẽ làm dữ liệu mất đi sự đồng nhất.\n",
    "- Cần thử nghiệm và chọn những augmentation thực sự hữu ích.\n",
    "- Nếu dataset nhỏ → Nên thử nhiều loại augmentation.\n",
    "- Nếu dataset đã lớn → Chỉ dùng augmentation nhẹ như Jittering, Rotation, Scaling.\n",
    "    * Jittering (0.01) + Rotation (±15°) + Temporal Warping (0.9 - 1.1). (Đúng)\n",
    "    * Jittering (0.1) + Rotation (±45°) + Scaling (0.5 - 1.5) → Quá mạnh, có thể làm hỏng dữ liệu. (Sai)\n",
    "3. Augmentation phải phù hợp với bài toán\n",
    "- Cần hiểu rõ bài toán và dữ liệu để chọn augmentation phù hợp.\n",
    "- Đối với Yoga: Tư thế có thể rất quan trọng, Horizontal Flip có thể không hợp lý.\n",
    "- Đối với hành động thể thao: Temporal Warping có thể giúp mô hình học tốc độ thực hiện khác nhau. \n",
    "- Đúng:\n",
    "    * Nếu tư thế Yoga có thể lật trái/phải → Dùng Horizontal Flip.\n",
    "    * Nếu cần học tốc độ thực hiện khác nhau → Dùng Temporal Warping.\n",
    "    * Nếu muốn mô hình mạnh hơn với dữ liệu nhiễu → Dùng Keypoint Masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def jittering(skeleton, noise_level=0.01):\n",
    "    \"\"\" Thêm nhiễu ngẫu nhiên vào keypoints \"\"\"\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=skeleton.shape)\n",
    "    return skeleton + noise\n",
    "\n",
    "def scaling(skeleton, scale_range=(0.9, 1.1)):\n",
    "    \"\"\" Phóng to/thu nhỏ keypoints \"\"\"\n",
    "    scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "    return skeleton * scale_factor\n",
    "\n",
    "def rotation(skeleton, angle_range=(-15, 15)):\n",
    "    \"\"\" Xoay keypoints quanh trung tâm \"\"\"\n",
    "    angle = np.radians(np.random.uniform(angle_range[0], angle_range[1]))\n",
    "    cos_val, sin_val = np.cos(angle), np.sin(angle)\n",
    "\n",
    "    # Tạo ma trận xoay\n",
    "    rotation_matrix = np.array([[cos_val, -sin_val], [sin_val, cos_val]])\n",
    "\n",
    "    # Chỉ xoay x, y (không xoay z)\n",
    "    skeleton[:, :, :2] = np.dot(skeleton[:, :, :2] - np.mean(skeleton[:, :, :2], axis=0), rotation_matrix) + np.mean(skeleton[:, :, :2], axis=0)\n",
    "    \n",
    "    return skeleton\n",
    "\n",
    "def horizontal_flip(skeleton):\n",
    "    \"\"\" Lật keypoints trái/phải \"\"\"\n",
    "    skeleton[:, :, 0] = 1 - skeleton[:, :, 0]  # Đảo ngược trục x\n",
    "    return skeleton\n",
    "\n",
    "def temporal_warping(skeleton, warp_factor_range=(0.8, 1.2)):\n",
    "    \"\"\" Tăng/giảm tốc độ chuyển động (thay đổi số frame) \"\"\"\n",
    "    warp_factor = np.random.uniform(warp_factor_range[0], warp_factor_range[1])\n",
    "    num_frames = int(skeleton.shape[0] * warp_factor)\n",
    "    indices = np.linspace(0, skeleton.shape[0] - 1, num_frames, dtype=int)\n",
    "    return skeleton[indices]\n",
    "\n",
    "def time_masking(skeleton, mask_ratio=0.2):\n",
    "    \"\"\" Ngẫu nhiên bỏ qua một số frame trong video \"\"\"\n",
    "    T = skeleton.shape[0]\n",
    "    num_mask = int(T * mask_ratio)\n",
    "    mask_indices = np.random.choice(T, num_mask, replace=False)\n",
    "    skeleton[mask_indices] = 0  # Gán toàn bộ frame đó về 0\n",
    "    return skeleton\n",
    "\n",
    "def frame_interpolation(skeleton):\n",
    "    \"\"\" Thêm frame nội suy giữa các frame hiện tại \"\"\"\n",
    "    T, V, C = skeleton.shape\n",
    "    new_T = T * 2 - 1  # Tăng gấp đôi số frame\n",
    "    interpolated_skeleton = np.zeros((new_T, V, C))\n",
    "\n",
    "    for i in range(T - 1):\n",
    "        interpolated_skeleton[2 * i] = skeleton[i]\n",
    "        interpolated_skeleton[2 * i + 1] = (skeleton[i] + skeleton[i + 1]) / 2  # Nội suy\n",
    "    \n",
    "    interpolated_skeleton[-1] = skeleton[-1]\n",
    "    return interpolated_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skeleton_with_augmentation(input_folder, output_folder, fps=10, augment=True):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for pose_name in os.listdir(input_folder):\n",
    "        pose_path = os.path.join(input_folder, pose_name)\n",
    "        output_pose_path = os.path.join(output_folder, pose_name)\n",
    "\n",
    "        if not os.path.exists(output_pose_path):\n",
    "            os.makedirs(output_pose_path)\n",
    "\n",
    "        for file in os.listdir(pose_path):\n",
    "            if file.endswith(\".mp4\"):\n",
    "                video_path = os.path.join(pose_path, file)\n",
    "                output_file = os.path.join(output_pose_path, file.replace(\".mp4\", \".npy\"))\n",
    "\n",
    "                if os.path.exists(output_file):\n",
    "                    continue  # Bỏ qua nếu đã trích xuất\n",
    "\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                selected_frames = np.arange(0, total_frames, step=int(video_fps / fps), dtype=int)\n",
    "                skeleton_data = []\n",
    "\n",
    "                for idx in selected_frames:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        continue\n",
    "\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    results = pose.process(frame_rgb)\n",
    "\n",
    "                    keypoints = []\n",
    "                    if results.pose_landmarks:\n",
    "                        for lm in results.pose_landmarks.landmark:\n",
    "                            keypoints.append([lm.x, lm.y, lm.z])\n",
    "                    else:\n",
    "                        keypoints = [[0, 0, 0]] * 33  # Nếu không nhận diện được, gán 0\n",
    "\n",
    "                    skeleton_data.append(keypoints)\n",
    "\n",
    "                cap.release()\n",
    "                skeleton_data = np.array(skeleton_data)  # Shape: (num_extracted_frames, 33, 3)\n",
    "\n",
    "                # Thực hiện augmentation nếu bật flag `augment`\n",
    "                if augment:\n",
    "                    skeleton_aug1 = jittering(skeleton_data)\n",
    "                    skeleton_aug2 = scaling(skeleton_data)\n",
    "                    skeleton_aug3 = rotation(skeleton_data)\n",
    "                    skeleton_aug4 = horizontal_flip(skeleton_data)\n",
    "                    skeleton_aug5 = temporal_warping(skeleton_data)\n",
    "                    skeleton_aug6 = time_masking(skeleton_data)\n",
    "                    skeleton_aug7 = frame_interpolation(skeleton_data)\n",
    "\n",
    "                    # Lưu augmentation vào file mới\n",
    "                    np.save(output_file.replace(\".npy\", \"_jitter.npy\"), skeleton_aug1)\n",
    "                    np.save(output_file.replace(\".npy\", \"_scale.npy\"), skeleton_aug2)\n",
    "                    np.save(output_file.replace(\".npy\", \"_rotate.npy\"), skeleton_aug3)\n",
    "                    np.save(output_file.replace(\".npy\", \"_flip.npy\"), skeleton_aug4)\n",
    "                    np.save(output_file.replace(\".npy\", \"_warp.npy\"), skeleton_aug5)\n",
    "                    np.save(output_file.replace(\".npy\", \"_mask.npy\"), skeleton_aug6)\n",
    "                    np.save(output_file.replace(\".npy\", \"_interpolate.npy\"), skeleton_aug7)\n",
    "\n",
    "                # Lưu file gốc\n",
    "                np.save(output_file, skeleton_data)\n",
    "\n",
    "                print(f\"Processed {video_path} -> {output_file} | Extracted Frames: {skeleton_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chạy trích xuất skeleton không có augmentation\n",
    "extract_skeleton_with_augmentation(\"Yoga_9gb\", \"Skeleton_data_9gb\", fps=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chạy trích xuất skeleton với augmentation\n",
    "extract_skeleton_with_augmentation(\"Yoga_9gb\", \"Skeleton_data_9gb_augmented\", fps=10, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHƯƠNG PHÁP MEAN CENTERING NORMALIZATION\n",
    "- Tính trung bình của tất cả keypoints trong toàn bộ video trên tọa độ x và y.\n",
    "- Dịch tất cả keypoints về trung tâm (0,0) bằng cách trừ đi trung bình này.\n",
    "\n",
    "Mục đích: \n",
    "- Giúp mô hình không bị ảnh hưởng bởi vị trí tuyệt đối của đối tượng.\n",
    "    * Nếu người thực hiện động tác đứng lệch trái/phải trong video, tọa độ tuyệt đối sẽ khác, nhưng động tác vẫn giống nhau.\n",
    "    * Normalize giúp mô hình chỉ tập trung vào hình dạng động tác, không phải vị trí. \n",
    "- Tạo sự đồng nhất giữa các video\n",
    "    * Nếu video 1 có người ở góc trên bên phải, video 2 có người ở trung tâm, thì mô hình có thể gặp khó khăn khi học.\n",
    "    * Normalize giúp đồng nhất dữ liệu, làm cho mô hình dễ học hơn. \n",
    "- Loại bỏ ảnh hưởng của vị trí camera\n",
    "    * Nếu camera đặt ở nhiều góc khác nhau, tọa độ keypoints có thể lệch nhưng động tác vẫn giống nhau.\n",
    "    * Chuẩn hóa giúp mô hình không phụ thuộc vào góc nhìn camera.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "input shape: (num_frames, num_keypoints, 3) --> (T, 33, 3)\n",
    "    - T: số lượng frame\n",
    "    - 33: số lượng keypoints\n",
    "    - 3: tọa độ x, y, z của keypoints\n",
    "Công thức toán học: \n",
    "    - mean(x) = 1/T*33 * sum(t=1->T, sum(i=1->33, x_t,i))\n",
    "    - mean(y) = 1/T*33 * sum(t=1->T, sum(i=1->33, y_t,i))\n",
    "    - skeletion(t,i,x) = x_t,i - mean(x)\n",
    "    - skeletion(t,i,y) = y_t,i - mean(y)\n",
    "    --> Mỗi keypoint (x, y) được trừ đi trung bình của toàn bộ keypoints\n",
    "\"\"\"\n",
    "\n",
    "def normalize_skeleton(skeleton):\n",
    "    \"\"\"Chuẩn hóa skeleton về trung tâm bằng cách trừ đi tọa độ trung bình.\"\"\"\n",
    "    mean_pose = np.mean(skeleton[:, :, :2], axis=(0, 1))  # Trung bình trên trục x, y\n",
    "    skeleton[:, :, :2] -= mean_pose  # Dịch keypoints về trung tâm\n",
    "    return skeleton\n",
    "\n",
    "input_folder = \"Skeleton_data_9gb\"\n",
    "# input_folder = \"Skeleton_data_9gb_augmented\"\n",
    "\n",
    "if not os.path.exists(input_folder):\n",
    "    raise FileNotFoundError(f\"Thư mục {input_folder} không tồn tại!\")\n",
    "\n",
    "for pose_name in os.listdir(input_folder):\n",
    "    pose_path = os.path.join(input_folder, pose_name)\n",
    "    \n",
    "    if not os.path.isdir(pose_path):  # Bỏ qua nếu không phải thư mục\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(pose_path):\n",
    "        if file.endswith(\".npy\"):\n",
    "            file_path = os.path.join(pose_path, file)\n",
    "            \n",
    "            # Load skeleton\n",
    "            skeleton = np.load(file_path)\n",
    "\n",
    "            # Chuẩn hóa\n",
    "            skeleton = normalize_skeleton(skeleton)\n",
    "\n",
    "            # Ghi đè file sau khi chuẩn hóa\n",
    "            np.save(file_path, skeleton)\n",
    "            print(f\"✅ Normalized: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class YogaSkeletonDataset(Dataset):\n",
    "    def __init__(self, root_dir, classes, T_max=100):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.T_max = T_max\n",
    "\n",
    "        for label, cls in enumerate(classes):\n",
    "            class_dir = os.path.join(root_dir, cls)\n",
    "            for file in os.listdir(class_dir):\n",
    "                if file.endswith(\".npy\"):\n",
    "                    self.video_paths.append(os.path.join(class_dir, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def pad_skeleton(self, skeleton):\n",
    "        T, V, C = skeleton.shape\n",
    "        if T < self.T_max:\n",
    "            pad = np.zeros((self.T_max - T, V, C))\n",
    "            skeleton = np.concatenate((skeleton, pad), axis=0)\n",
    "        else:\n",
    "            skeleton = skeleton[:self.T_max]\n",
    "        return skeleton\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        skeleton = np.load(self.video_paths[idx])\n",
    "        skeleton = self.pad_skeleton(skeleton)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(skeleton, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "# Tạo DataLoader\n",
    "classes = [\"Garland_Pose\", \"Happy_Baby_Pose\", \"Head_To_Knee_Pose\", \"Lunge_Pose\",\n",
    "           \"Mountain_Pose\", \"Plank_Pose\", \"Raised_Arms_Pose\", \"Seated_Forward_Bend\",\n",
    "           \"Staff_Pose\", \"Standing_Forward_Bend\"]\n",
    "\n",
    "train_dataset = YogaSkeletonDataset(\"Skeleton_data_9gb\", classes, T_max=100)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# train_dataset = YogaSkeletonDataset(\"Skeleton_data_9gb_augmented\", classes, T_max=100)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Kiểm tra dữ liệu\n",
    "skeleton_sample, label_sample = train_dataset[0]\n",
    "print(\"Sample shape:\", skeleton_sample.shape)  # Expect (100, 33, 3)\n",
    "print(\"Label:\", classes[label_sample.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Skeleton_data_9gb\"  # Chọn bộ gốc\n",
    "# dataset_path = \"Skeleton_data_9gb_augmented\"  # Chọn bộ augmented\n",
    "\n",
    "video_paths = []\n",
    "labels = []\n",
    "classes = sorted(os.listdir(dataset_path))  # Danh sách động tác\n",
    "\n",
    "train_videos = []\n",
    "train_labels = []\n",
    "valid_videos = []\n",
    "valid_labels = []\n",
    "\n",
    "for label, cls in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_path, cls)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    # Lấy danh sách file .npy trong thư mục của động tác hiện tại\n",
    "    files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "    # Xáo trộn danh sách file để đảm bảo train/valid ngẫu nhiên\n",
    "    np.random.shuffle(files)\n",
    "\n",
    "    # Chia theo tỷ lệ 80% train / 20% validation\n",
    "    split_idx = int(0.8 * len(files))\n",
    "    train_videos.extend(files[:split_idx])\n",
    "    train_labels.extend([label] * split_idx)\n",
    "    valid_videos.extend(files[split_idx:])\n",
    "    valid_labels.extend([label] * (len(files) - split_idx))\n",
    "\n",
    "# Tạo dataset và dataloader cho train và validation\n",
    "train_dataset = YogaSkeletonDataset(train_videos, train_labels, T_max=100)\n",
    "valid_dataset = YogaSkeletonDataset(valid_videos, valid_labels, T_max=100)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Kiểm tra số lượng mẫu trong train/valid\n",
    "print(f\"📌 Tổng số mẫu: {len(train_dataset) + len(valid_dataset)}\")\n",
    "print(f\"✅ Train set: {len(train_dataset)} samples\")\n",
    "print(f\"✅ Validation set: {len(valid_dataset)} samples\")\n",
    "\n",
    "# Kiểm tra một mẫu trong train set\n",
    "skeleton_sample, label_sample = train_dataset[0]\n",
    "print(\"Sample shape:\", skeleton_sample.shape)  # Kỳ vọng: (100, 33, 3)\n",
    "print(\"Label:\", classes[label_sample.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class YogaGCN(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_dim=64, num_classes=10):\n",
    "        super(YogaGCN, self).__init__()\n",
    "        self.conv1 = gnn.GCNConv(in_channels, hidden_dim)\n",
    "        self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = torch.mean(x, dim=0)  # Pooling để lấy đặc trưng toàn video\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# V2: thử tăng kích thước nếu model v1 lỏ    \n",
    "# class YogaGCN(nn.Module):\n",
    "#     def __init__(self, in_channels=3, hidden_dim=128, num_classes=10):\n",
    "#         super(YogaGCN, self).__init__()\n",
    "#         self.conv1 = gnn.GCNConv(in_channels, hidden_dim)\n",
    "#         self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.conv3 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.conv4 = gnn.GCNConv(hidden_dim, hidden_dim)  # Thêm một lớp nữa\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "#         x = self.conv3(x, edge_index).relu()\n",
    "#         x = self.conv4(x, edge_index).relu()\n",
    "#         x = torch.mean(x, dim=0)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# V3: thêm dropout\n",
    "# class YogaGCN(nn.Module):\n",
    "#     def __init__(self, in_channels=3, hidden_dim=64, num_classes=10):\n",
    "#         super(YogaGCN, self).__init__()\n",
    "#         self.conv1 = gnn.GCNConv(in_channels, hidden_dim)\n",
    "#         self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.conv3 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     self.dropout = nn.Dropout(0.3)  # 30% dropout\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.conv3(x, edge_index).relu()\n",
    "#         x = torch.mean(x, dim=0)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "# V4: Dùng GAT (Graph Attention Network), chậm hơn cần GPU mạnh hơn\n",
    "# class YogaGAT(nn.Module):\n",
    "#     def __init__(self, in_channels=3, hidden_dim=128, num_classes=10):\n",
    "#         super(YogaGAT, self).__init__()\n",
    "#         self.conv1 = gnn.GATConv(in_channels, hidden_dim, heads=4, concat=True)\n",
    "#         self.conv2 = gnn.GATConv(hidden_dim * 4, hidden_dim, heads=4, concat=True)\n",
    "#         self.conv3 = gnn.GATConv(hidden_dim * 4, hidden_dim, heads=4, concat=True)\n",
    "#         self.fc = nn.Linear(hidden_dim * 4, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "#         x = self.conv3(x, edge_index).relu()\n",
    "#         x = torch.mean(x, dim=0)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Danh sách kết nối giữa các node (khớp xương) dựa trên Mediapipe Pose\n",
    "edges = [\n",
    "    (0, 1), (1, 2), (2, 3), (0, 4), (4, 5), (5, 6), (0, 7), (0, 8), (7, 9), (8, 10),\n",
    "    (11, 12), (11, 23), (12, 24), (23, 24), (11, 13), (13, 15), (15, 17), (15, 19), (15, 21),\n",
    "    (12, 14), (14, 16), (16, 18), (16, 20), (16, 22), (23, 25), (25, 27), (27, 29), (29, 31),\n",
    "    (24, 26), (26, 28), (28, 30), (30, 32)\n",
    "]\n",
    "\n",
    "# Chuyển thành tensor cho PyTorch Geometric\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gcn(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.squeeze(1), edge_index.to(device))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return total_loss / len(train_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gcn(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs.squeeze(1), edge_index.to(device))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return total_loss / len(valid_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YogaGCN(in_channels=3, hidden_dim=64, num_classes=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"📌 Số lượng tham số của YogaGCN: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # Số epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_gcn(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate_gcn(model, valid_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"yoga_gcn_model.pth\")\n",
    "print(\"✅ Mô hình đã được lưu thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(model, video_path, classes, device):\n",
    "    model.eval()\n",
    "    skeleton = np.load(video_path)  # Load skeleton đã trích xuất\n",
    "    skeleton = torch.tensor(skeleton, dtype=torch.float32).to(device).unsqueeze(0)  # Thêm batch dim\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(skeleton.squeeze(1), edge_index.to(device))\n",
    "        _, pred = torch.max(output, 1)\n",
    "    \n",
    "    return classes[pred.item()]\n",
    "\n",
    "# Test với một video\n",
    "video_path = \"skeleton_data/test/Garland_Pose/video_13.npy\"\n",
    "predicted_class = predict_video(model, video_path, classes, device)\n",
    "print(f\"✅ Predicted Class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
