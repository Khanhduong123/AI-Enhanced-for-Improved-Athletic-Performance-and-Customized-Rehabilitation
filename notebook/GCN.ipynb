{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√ÅC PH∆Ø∆†NG PH√ÅP AUGEMENTATION\n",
    "\n",
    "1. JITTERING (th√™m nhi·ªÖu v√†o c√°c keypoints)\n",
    "- Th√™m nhi·ªÖu guassian (norm distribution) v√†o c√°c keypoints\n",
    "- gi√∫p m√¥ h√¨nh kh√¥ng b·ªã ph·ª• thu·ªôc v√†o v·ªã tr√≠ ch√≠nh x√°c c·ªßa keypoints\n",
    "- gi√∫p m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c c√°c bi·∫øn th·ªÉ c·ªßa keypoints\n",
    "- c√¥ng th·ª©c: skeleton(aug) = skeleton + N(0, sigma^2) v·ªõi N(0, sigma^2) l√† ph√¢n ph·ªëi Guassian c√≥ trung b√¨nh 0 v√† ƒë·ªô l·ªách chu·∫©n sigma\n",
    "- D√πng khi n√†o? Khi mu·ªën m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c s·ª± bi·∫øn ƒë·ªïi nh·ªè trong t·ªça ƒë·ªô keypoints, khi d·ªØ li·ªáu qu√° s·∫°ch v√† d·ªÖ b·ªã overfitting\n",
    "- VD: ban ƒë·∫ßu: (0.5, 0.3, 0.2) --> jittering (v·ªõi noise nh·ªè): (0.502, 0.295, 0.205)\n",
    "2. Scaling (ph√≥ng to/ thu nh·ªè to√†n b·ªô skeleton)\n",
    "- Ph√≥ng to ho·∫∑c thu nh·ªè keypoints theo m·ªôt h·ªá s·ªë ng·∫´u nhi√™n.\n",
    "- Gi√∫p m√¥ h√¨nh kh√¥ng b·ªã ph·ª• thu·ªôc v√†o k√≠ch th∆∞·ªõc ng∆∞·ªùi t·∫≠p yoga.\n",
    "- C√¥ng th·ª©c: skeleton(aug) = skeleton * scale_factor (trong n√†y set l√† trong kho·∫£ng 0.9 v√† 1.1)\n",
    "- D√πng khi n√†o? Khi mu·ªën m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c s·ª± bi·∫øn ƒë·ªïi v·ªÅ k√≠ch th∆∞·ªõc c·ªßa ng∆∞·ªùi t·∫≠p yoga (cao th·∫•p, m·∫≠p ·ªëm,...) --> kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi k√≠ch th∆∞·ªõc tuy·ªát ƒë·ªëi\n",
    "- VD: ban ƒë·∫ßu: (0.5, 0.3, 0.2) --> scaling (scale_factor = 1.1): (0.55, 0.33, 0.22)\n",
    "3. Rotation (Xoay to√†n b·ªô skeleton quanh trung t√¢m)\n",
    "- Xoay khung x∆∞∆°ng quanh trung t√¢m theo g√≥c ng·∫´u nhi√™n (-15 ƒë·∫øn 15 ƒë·ªô)\n",
    "- Gi√∫p m√¥ h√¨nh hi·ªÉu ƒë∆∞·ª£c c√°c t∆∞ th·∫ø nh√¨n t·ª´ nhi·ªÅu g√≥c ƒë·ªô kh√°c nhau.\n",
    "- C√¥ng th·ª©c: \n",
    "    * R = [[cos(theta), -sin(theta)], [sin(theta), cos(theta)]] v·ªõi theta l√† g√≥c xoay\n",
    "    * skeleton(aug) = R * (skeleton - center)*R + center\n",
    "- D√πng khi n√†o? Khi mu·ªën m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c s·ª± bi·∫øn ƒë·ªïi v·ªÅ g√≥c nh√¨n c·ªßa ng∆∞·ªùi t·∫≠p yoga, ph√≤ng khi data ch·ªâ thu ƒë∆∞·ª£c t·ª´ m·ªôt goÃÅc ƒë·ªô c·ªë ƒë·ªãnh\n",
    "- VD: N·∫øu ƒëi·ªÉm tay ban ƒë·∫ßu: (0.5, 0.3) --> xoay 10 ƒë·ªô (0.48, 0.32)\n",
    "4. Horizontal Flip (L·∫≠t keypoints tr√°i/ph·∫£i)\n",
    "- L·∫≠t to√†n b·ªô t∆∞ th·∫ø theo tr·ª•c X\n",
    "- Gi√∫p m√¥ h√¨nh hi·ªÉu c·∫£ t∆∞ th·∫ø t·ª´ c·∫£ hai ph√≠a tr√°i ph·∫£i c·ªßa c√πng m·ªôt ƒë·ªông t√°c\n",
    "- c√¥ng th·ª©c: skeleton(aug) = [:, :, 0] = 1 - skeleton[:, :, 0]\n",
    "- D√πng khi n√†o? Khi mu·ªën m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c s·ª± bi·∫øn ƒë·ªïi v·ªÅ tr√°i ph·∫£i c·ªßa ng∆∞·ªùi t·∫≠p yoga, khi d·ªØ li·ªáu ch·ªâ c√≥ m·ªôt phi√™n b·∫£n c·ªßa ƒë·ªông t√°c\n",
    "- VD: ban ƒë·∫ßu (0.2, 0.5) --> flip (0.8, 0.5)\n",
    "5. Temporal Warping (TƒÉng/Gi·∫£m t·ªëc ƒë·ªô chuy·ªÉn ƒë·ªông)\n",
    "- TƒÉng ho·∫∑c gi·∫£m t·ªëc ƒë·ªô c·ªßa video m√† kh√¥ng l√†m m·∫•t ƒë·ªông t√°c.\n",
    "- Gi√∫p m√¥ h√¨nh hi·ªÉu ƒë∆∞·ª£c ƒë·ªông t√°c th·ª±c hi·ªán nhanh/ch·∫≠m kh√°c nhau.\n",
    "- c√¥ng th·ª©c: \n",
    "    * num_frames = T * warp_factor\n",
    "    * skeleton(aug) = skeleton[indices]\n",
    "    * v·ªõi warp_factor (0.8, 1.2) l√† h·ªá s·ªë t·ªëc ƒë·ªô, indices l√† c√°c ch·ªâ s·ªë c·ªßa frame sau khi tƒÉng/gi·∫£m t·ªëc ƒë·ªô\n",
    "- D√πng khi n√†o? Khi ƒë·ªông t√°c c√≥ th·ªÉ th·ª±c hi·ªán v·ªõi t·ªëc ƒë·ªô kh√°c nhau, khi video c√≥ frame_rate kh√¥ng ƒë·ªìng ƒë·ªÅu\n",
    "- VD: ban ƒë·∫ßu: 10 frames --> gi·∫£m t·ªëc ƒë·ªô (warp_factor = 1.2): 12 frames\n",
    "6. Time Masking (·∫®n m·ªôt s·ªë frame)\n",
    "- Lo·∫°i b·ªè m·ªôt s·ªë frame ng·∫´u nhi√™n, gi·∫£ l·∫≠p video b·ªã m·∫•t frame ho·∫∑c h√†nh ƒë·ªông kh√¥ng li√™n t·ª•c.\n",
    "- Gi√∫p m√¥ h√¨nh h·ªçc c√°ch ƒë·ªëi ph√≥ v·ªõi d·ªØ li·ªáu kh√¥ng li√™n t·ª•c.\n",
    "- D√πng khi n√†o? Khi video c√≥ t·ªëc ƒë·ªô ghi h√¨nh kh√¥ng ·ªïn ƒë·ªãnh, khi mu·ªën m√¥ h√¨nh kh√¥ng b·ªã ph·ª• thu·ªôc v√†o to√†n b·ªô chu·ªói frame.\n",
    "- VD: ban ƒë·∫ßu: [F1, F2, F3, F4, F5] --> Sau masking: [F1, 0, F3, 0, F5]\n",
    "7. Frame Interpolation (N·ªôi suy khung h√¨nh)\n",
    "- T·∫°o th√™m frame n·ªôi suy gi·ªØa c√°c frame th·ª±c t·∫ø, gi√∫p m√¥ h√¨nh h·ªçc m∆∞·ª£t h∆°n.\n",
    "- Gi·∫£ l·∫≠p d·ªØ li·ªáu c√≥ t·ªëc ƒë·ªô quay cao h∆°n.\n",
    "- D√πng khi n√†o? Khi video b·ªã √≠t frame, c·∫ßn t·∫°o th√™m frame ƒë·ªÉ training, khi mu·ªën m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c chuy·ªÉn ƒë·ªông m∆∞·ª£t m√† h∆°n.\n",
    "- VD: ban ƒë·∫ßu: [F1, F2, F3] --> Interpolation: [F1, F1.5, F2, F2.5, F3]\n",
    "\n",
    "==================================K·∫æT LU·∫¨N================================\n",
    "- N·∫øu mu·ªën tƒÉng ƒë·ªô ƒëa d·∫°ng, th·ª≠ Jittering + Rotation + Scaling.\n",
    "- N·∫øu d·ªØ li·ªáu b·ªã thi·∫øu keypoints, th·ª≠ Keypoint Masking + Time Masking.\n",
    "- N·∫øu d·ªØ li·ªáu c√≥ t·ªëc ƒë·ªô kh√°c nhau, th·ª≠ Temporal Warping + Frame Interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUGMENT TH·∫æ N√ÄO CHO ƒê√öNG?\n",
    "1. Augmentation ph·∫£i gi·ªØ nguy√™n b·∫£n ch·∫•t d·ªØ li·ªáu\n",
    "- Augmentation kh√¥ng ƒë∆∞·ª£c l√†m m·∫•t ƒëi c·∫•u tr√∫c ƒë·ªông t√°c.\n",
    "    * Horizontal Flip c√≥ th·ªÉ sai n·∫øu ƒë·ªông t√°c ch·ªâ c√≥ m·ªôt chi·ªÅu nh·∫•t ƒë·ªãnh. (Sai)\n",
    "    * Rotation qu√° m·∫°nh (>30¬∞) c√≥ th·ªÉ l√†m sai t∆∞ th·∫ø. (Sai)\n",
    "    * Jittering ch·ªâ n√™n th√™m nhi·ªÖu nh·ªè (¬±0.01) ƒë·ªÉ tr√°nh l√†m thay ƒë·ªïi v·ªã tr√≠ ƒë·ªông t√°c. (ƒê√∫ng)\n",
    "    * Rotation ch·ªâ n√™n xoay trong kho·∫£ng ¬±15¬∞ ƒë·ªÉ gi·ªØ ƒë√∫ng c·∫•u tr√∫c khung x∆∞∆°ng. (ƒê√∫ng)\n",
    "2. Augmentation ph·∫£i ƒëa d·∫°ng nh∆∞ng kh√¥ng g√¢y nhi·ªÖu qu√° m·ª©c\n",
    "- Kh√¥ng n√™n √°p d·ª•ng qu√° nhi·ªÅu augmentation c√πng l√∫c, v√¨ s·∫Ω l√†m d·ªØ li·ªáu m·∫•t ƒëi s·ª± ƒë·ªìng nh·∫•t.\n",
    "- C·∫ßn th·ª≠ nghi·ªám v√† ch·ªçn nh·ªØng augmentation th·ª±c s·ª± h·ªØu √≠ch.\n",
    "- N·∫øu dataset nh·ªè ‚Üí N√™n th·ª≠ nhi·ªÅu lo·∫°i augmentation.\n",
    "- N·∫øu dataset ƒë√£ l·ªõn ‚Üí Ch·ªâ d√πng augmentation nh·∫π nh∆∞ Jittering, Rotation, Scaling.\n",
    "    * Jittering (0.01) + Rotation (¬±15¬∞) + Temporal Warping (0.9 - 1.1). (ƒê√∫ng)\n",
    "    * Jittering (0.1) + Rotation (¬±45¬∞) + Scaling (0.5 - 1.5) ‚Üí Qu√° m·∫°nh, c√≥ th·ªÉ l√†m h·ªèng d·ªØ li·ªáu. (Sai)\n",
    "3. Augmentation ph·∫£i ph√π h·ª£p v·ªõi b√†i to√°n\n",
    "- C·∫ßn hi·ªÉu r√µ b√†i to√°n v√† d·ªØ li·ªáu ƒë·ªÉ ch·ªçn augmentation ph√π h·ª£p.\n",
    "- ƒê·ªëi v·ªõi Yoga: T∆∞ th·∫ø c√≥ th·ªÉ r·∫•t quan tr·ªçng, Horizontal Flip c√≥ th·ªÉ kh√¥ng h·ª£p l√Ω.\n",
    "- ƒê·ªëi v·ªõi h√†nh ƒë·ªông th·ªÉ thao: Temporal Warping c√≥ th·ªÉ gi√∫p m√¥ h√¨nh h·ªçc t·ªëc ƒë·ªô th·ª±c hi·ªán kh√°c nhau. \n",
    "- ƒê√∫ng:\n",
    "    * N·∫øu t∆∞ th·∫ø Yoga c√≥ th·ªÉ l·∫≠t tr√°i/ph·∫£i ‚Üí D√πng Horizontal Flip.\n",
    "    * N·∫øu c·∫ßn h·ªçc t·ªëc ƒë·ªô th·ª±c hi·ªán kh√°c nhau ‚Üí D√πng Temporal Warping.\n",
    "    * N·∫øu mu·ªën m√¥ h√¨nh m·∫°nh h∆°n v·ªõi d·ªØ li·ªáu nhi·ªÖu ‚Üí D√πng Keypoint Masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def jittering(skeleton, noise_level=0.01):\n",
    "    \"\"\" Th√™m nhi·ªÖu ng·∫´u nhi√™n v√†o keypoints \"\"\"\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=skeleton.shape)\n",
    "    return skeleton + noise\n",
    "\n",
    "def scaling(skeleton, scale_range=(0.9, 1.1)):\n",
    "    \"\"\" Ph√≥ng to/thu nh·ªè keypoints \"\"\"\n",
    "    scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "    return skeleton * scale_factor\n",
    "\n",
    "def rotation(skeleton, angle_range=(-15, 15)):\n",
    "    \"\"\" Xoay keypoints quanh trung t√¢m \"\"\"\n",
    "    angle = np.radians(np.random.uniform(angle_range[0], angle_range[1]))\n",
    "    cos_val, sin_val = np.cos(angle), np.sin(angle)\n",
    "\n",
    "    # T·∫°o ma tr·∫≠n xoay\n",
    "    rotation_matrix = np.array([[cos_val, -sin_val], [sin_val, cos_val]])\n",
    "\n",
    "    # Ch·ªâ xoay x, y (kh√¥ng xoay z)\n",
    "    skeleton[:, :, :2] = np.dot(skeleton[:, :, :2] - np.mean(skeleton[:, :, :2], axis=0), rotation_matrix) + np.mean(skeleton[:, :, :2], axis=0)\n",
    "    \n",
    "    return skeleton\n",
    "\n",
    "def horizontal_flip(skeleton):\n",
    "    \"\"\" L·∫≠t keypoints tr√°i/ph·∫£i \"\"\"\n",
    "    skeleton[:, :, 0] = 1 - skeleton[:, :, 0]  # ƒê·∫£o ng∆∞·ª£c tr·ª•c x\n",
    "    return skeleton\n",
    "\n",
    "def temporal_warping(skeleton, warp_factor_range=(0.8, 1.2)):\n",
    "    \"\"\" TƒÉng/gi·∫£m t·ªëc ƒë·ªô chuy·ªÉn ƒë·ªông (thay ƒë·ªïi s·ªë frame) \"\"\"\n",
    "    warp_factor = np.random.uniform(warp_factor_range[0], warp_factor_range[1])\n",
    "    num_frames = int(skeleton.shape[0] * warp_factor)\n",
    "    indices = np.linspace(0, skeleton.shape[0] - 1, num_frames, dtype=int)\n",
    "    return skeleton[indices]\n",
    "\n",
    "def time_masking(skeleton, mask_ratio=0.2):\n",
    "    \"\"\" Ng·∫´u nhi√™n b·ªè qua m·ªôt s·ªë frame trong video \"\"\"\n",
    "    T = skeleton.shape[0]\n",
    "    num_mask = int(T * mask_ratio)\n",
    "    mask_indices = np.random.choice(T, num_mask, replace=False)\n",
    "    skeleton[mask_indices] = 0  # G√°n to√†n b·ªô frame ƒë√≥ v·ªÅ 0\n",
    "    return skeleton\n",
    "\n",
    "def frame_interpolation(skeleton):\n",
    "    \"\"\" Th√™m frame n·ªôi suy gi·ªØa c√°c frame hi·ªán t·∫°i \"\"\"\n",
    "    T, V, C = skeleton.shape\n",
    "    new_T = T * 2 - 1  # TƒÉng g·∫•p ƒë√¥i s·ªë frame\n",
    "    interpolated_skeleton = np.zeros((new_T, V, C))\n",
    "\n",
    "    for i in range(T - 1):\n",
    "        interpolated_skeleton[2 * i] = skeleton[i]\n",
    "        interpolated_skeleton[2 * i + 1] = (skeleton[i] + skeleton[i + 1]) / 2  # N·ªôi suy\n",
    "    \n",
    "    interpolated_skeleton[-1] = skeleton[-1]\n",
    "    return interpolated_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skeleton_with_augmentation(input_folder, output_folder, fps=10, augment=True):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for pose_name in os.listdir(input_folder):\n",
    "        pose_path = os.path.join(input_folder, pose_name)\n",
    "        output_pose_path = os.path.join(output_folder, pose_name)\n",
    "\n",
    "        if not os.path.exists(output_pose_path):\n",
    "            os.makedirs(output_pose_path)\n",
    "\n",
    "        for file in os.listdir(pose_path):\n",
    "            if file.endswith(\".mp4\"):\n",
    "                video_path = os.path.join(pose_path, file)\n",
    "                output_file = os.path.join(output_pose_path, file.replace(\".mp4\", \".npy\"))\n",
    "\n",
    "                if os.path.exists(output_file):\n",
    "                    continue  # B·ªè qua n·∫øu ƒë√£ tr√≠ch xu·∫•t\n",
    "\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                selected_frames = np.arange(0, total_frames, step=int(video_fps / fps), dtype=int)\n",
    "                skeleton_data = []\n",
    "\n",
    "                for idx in selected_frames:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        continue\n",
    "\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    results = pose.process(frame_rgb)\n",
    "\n",
    "                    keypoints = []\n",
    "                    if results.pose_landmarks:\n",
    "                        for lm in results.pose_landmarks.landmark:\n",
    "                            keypoints.append([lm.x, lm.y, lm.z])\n",
    "                    else:\n",
    "                        keypoints = [[0, 0, 0]] * 33  # N·∫øu kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c, g√°n 0\n",
    "\n",
    "                    skeleton_data.append(keypoints)\n",
    "\n",
    "                cap.release()\n",
    "                skeleton_data = np.array(skeleton_data)  # Shape: (num_extracted_frames, 33, 3)\n",
    "\n",
    "                # Th·ª±c hi·ªán augmentation n·∫øu b·∫≠t flag `augment`\n",
    "                if augment:\n",
    "                    skeleton_aug1 = jittering(skeleton_data)\n",
    "                    skeleton_aug2 = scaling(skeleton_data)\n",
    "                    skeleton_aug3 = rotation(skeleton_data)\n",
    "                    skeleton_aug4 = horizontal_flip(skeleton_data)\n",
    "                    skeleton_aug5 = temporal_warping(skeleton_data)\n",
    "                    skeleton_aug6 = time_masking(skeleton_data)\n",
    "                    skeleton_aug7 = frame_interpolation(skeleton_data)\n",
    "\n",
    "                    # L∆∞u augmentation v√†o file m·ªõi\n",
    "                    np.save(output_file.replace(\".npy\", \"_jitter.npy\"), skeleton_aug1)\n",
    "                    np.save(output_file.replace(\".npy\", \"_scale.npy\"), skeleton_aug2)\n",
    "                    np.save(output_file.replace(\".npy\", \"_rotate.npy\"), skeleton_aug3)\n",
    "                    np.save(output_file.replace(\".npy\", \"_flip.npy\"), skeleton_aug4)\n",
    "                    np.save(output_file.replace(\".npy\", \"_warp.npy\"), skeleton_aug5)\n",
    "                    np.save(output_file.replace(\".npy\", \"_mask.npy\"), skeleton_aug6)\n",
    "                    np.save(output_file.replace(\".npy\", \"_interpolate.npy\"), skeleton_aug7)\n",
    "\n",
    "                # L∆∞u file g·ªëc\n",
    "                np.save(output_file, skeleton_data)\n",
    "\n",
    "                print(f\"Processed {video_path} -> {output_file} | Extracted Frames: {skeleton_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y tr√≠ch xu·∫•t skeleton kh√¥ng c√≥ augmentation\n",
    "extract_skeleton_with_augmentation(\"Yoga_9gb\", \"Skeleton_data_9gb\", fps=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y tr√≠ch xu·∫•t skeleton v·ªõi augmentation\n",
    "extract_skeleton_with_augmentation(\"Yoga_9gb\", \"Skeleton_data_9gb_augmented\", fps=10, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PH∆Ø∆†NG PH√ÅP MEAN CENTERING NORMALIZATION\n",
    "- T√≠nh trung b√¨nh c·ªßa t·∫•t c·∫£ keypoints trong to√†n b·ªô video tr√™n t·ªça ƒë·ªô x v√† y.\n",
    "- D·ªãch t·∫•t c·∫£ keypoints v·ªÅ trung t√¢m (0,0) b·∫±ng c√°ch tr·ª´ ƒëi trung b√¨nh n√†y.\n",
    "\n",
    "M·ª•c ƒë√≠ch: \n",
    "- Gi√∫p m√¥ h√¨nh kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi v·ªã tr√≠ tuy·ªát ƒë·ªëi c·ªßa ƒë·ªëi t∆∞·ª£ng.\n",
    "    * N·∫øu ng∆∞·ªùi th·ª±c hi·ªán ƒë·ªông t√°c ƒë·ª©ng l·ªách tr√°i/ph·∫£i trong video, t·ªça ƒë·ªô tuy·ªát ƒë·ªëi s·∫Ω kh√°c, nh∆∞ng ƒë·ªông t√°c v·∫´n gi·ªëng nhau.\n",
    "    * Normalize gi√∫p m√¥ h√¨nh ch·ªâ t·∫≠p trung v√†o h√¨nh d·∫°ng ƒë·ªông t√°c, kh√¥ng ph·∫£i v·ªã tr√≠. \n",
    "- T·∫°o s·ª± ƒë·ªìng nh·∫•t gi·ªØa c√°c video\n",
    "    * N·∫øu video 1 c√≥ ng∆∞·ªùi ·ªü g√≥c tr√™n b√™n ph·∫£i, video 2 c√≥ ng∆∞·ªùi ·ªü trung t√¢m, th√¨ m√¥ h√¨nh c√≥ th·ªÉ g·∫∑p kh√≥ khƒÉn khi h·ªçc.\n",
    "    * Normalize gi√∫p ƒë·ªìng nh·∫•t d·ªØ li·ªáu, l√†m cho m√¥ h√¨nh d·ªÖ h·ªçc h∆°n. \n",
    "- Lo·∫°i b·ªè ·∫£nh h∆∞·ªüng c·ªßa v·ªã tr√≠ camera\n",
    "    * N·∫øu camera ƒë·∫∑t ·ªü nhi·ªÅu g√≥c kh√°c nhau, t·ªça ƒë·ªô keypoints c√≥ th·ªÉ l·ªách nh∆∞ng ƒë·ªông t√°c v·∫´n gi·ªëng nhau.\n",
    "    * Chu·∫©n h√≥a gi√∫p m√¥ h√¨nh kh√¥ng ph·ª• thu·ªôc v√†o g√≥c nh√¨n camera.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "input shape: (num_frames, num_keypoints, 3) --> (T, 33, 3)\n",
    "    - T: s·ªë l∆∞·ª£ng frame\n",
    "    - 33: s·ªë l∆∞·ª£ng keypoints\n",
    "    - 3: t·ªça ƒë·ªô x, y, z c·ªßa keypoints\n",
    "C√¥ng th·ª©c to√°n h·ªçc: \n",
    "    - mean(x) = 1/T*33 * sum(t=1->T, sum(i=1->33, x_t,i))\n",
    "    - mean(y) = 1/T*33 * sum(t=1->T, sum(i=1->33, y_t,i))\n",
    "    - skeletion(t,i,x) = x_t,i - mean(x)\n",
    "    - skeletion(t,i,y) = y_t,i - mean(y)\n",
    "    --> M·ªói keypoint (x, y) ƒë∆∞·ª£c tr·ª´ ƒëi trung b√¨nh c·ªßa to√†n b·ªô keypoints\n",
    "\"\"\"\n",
    "\n",
    "def normalize_skeleton(skeleton):\n",
    "    \"\"\"Chu·∫©n h√≥a skeleton v·ªÅ trung t√¢m b·∫±ng c√°ch tr·ª´ ƒëi t·ªça ƒë·ªô trung b√¨nh.\"\"\"\n",
    "    mean_pose = np.mean(skeleton[:, :, :2], axis=(0, 1))  # Trung b√¨nh tr√™n tr·ª•c x, y\n",
    "    skeleton[:, :, :2] -= mean_pose  # D·ªãch keypoints v·ªÅ trung t√¢m\n",
    "    return skeleton\n",
    "\n",
    "input_folder = \"Skeleton_data_9gb\"\n",
    "# input_folder = \"Skeleton_data_9gb_augmented\"\n",
    "\n",
    "if not os.path.exists(input_folder):\n",
    "    raise FileNotFoundError(f\"Th∆∞ m·ª•c {input_folder} kh√¥ng t·ªìn t·∫°i!\")\n",
    "\n",
    "for pose_name in os.listdir(input_folder):\n",
    "    pose_path = os.path.join(input_folder, pose_name)\n",
    "    \n",
    "    if not os.path.isdir(pose_path):  # B·ªè qua n·∫øu kh√¥ng ph·∫£i th∆∞ m·ª•c\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(pose_path):\n",
    "        if file.endswith(\".npy\"):\n",
    "            file_path = os.path.join(pose_path, file)\n",
    "            \n",
    "            # Load skeleton\n",
    "            skeleton = np.load(file_path)\n",
    "\n",
    "            # Chu·∫©n h√≥a\n",
    "            skeleton = normalize_skeleton(skeleton)\n",
    "\n",
    "            # Ghi ƒë√® file sau khi chu·∫©n h√≥a\n",
    "            np.save(file_path, skeleton)\n",
    "            print(f\"‚úÖ Normalized: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class YogaSkeletonDataset(Dataset):\n",
    "    def __init__(self, root_dir, classes, T_max=100):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.T_max = T_max\n",
    "\n",
    "        for label, cls in enumerate(classes):\n",
    "            class_dir = os.path.join(root_dir, cls)\n",
    "            for file in os.listdir(class_dir):\n",
    "                if file.endswith(\".npy\"):\n",
    "                    self.video_paths.append(os.path.join(class_dir, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def pad_skeleton(self, skeleton):\n",
    "        T, V, C = skeleton.shape\n",
    "        if T < self.T_max:\n",
    "            pad = np.zeros((self.T_max - T, V, C))\n",
    "            skeleton = np.concatenate((skeleton, pad), axis=0)\n",
    "        else:\n",
    "            skeleton = skeleton[:self.T_max]\n",
    "        return skeleton\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        skeleton = np.load(self.video_paths[idx])\n",
    "        skeleton = self.pad_skeleton(skeleton)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(skeleton, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "# T·∫°o DataLoader\n",
    "classes = [\"Garland_Pose\", \"Happy_Baby_Pose\", \"Head_To_Knee_Pose\", \"Lunge_Pose\",\n",
    "           \"Mountain_Pose\", \"Plank_Pose\", \"Raised_Arms_Pose\", \"Seated_Forward_Bend\",\n",
    "           \"Staff_Pose\", \"Standing_Forward_Bend\"]\n",
    "\n",
    "train_dataset = YogaSkeletonDataset(\"Skeleton_data_9gb\", classes, T_max=100)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# train_dataset = YogaSkeletonDataset(\"Skeleton_data_9gb_augmented\", classes, T_max=100)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Ki·ªÉm tra d·ªØ li·ªáu\n",
    "skeleton_sample, label_sample = train_dataset[0]\n",
    "print(\"Sample shape:\", skeleton_sample.shape)  # Expect (100, 33, 3)\n",
    "print(\"Label:\", classes[label_sample.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Skeleton_data_9gb\"  # Ch·ªçn b·ªô g·ªëc\n",
    "# dataset_path = \"Skeleton_data_9gb_augmented\"  # Ch·ªçn b·ªô augmented\n",
    "\n",
    "video_paths = []\n",
    "labels = []\n",
    "classes = sorted(os.listdir(dataset_path))  # Danh s√°ch ƒë·ªông t√°c\n",
    "\n",
    "train_videos = []\n",
    "train_labels = []\n",
    "valid_videos = []\n",
    "valid_labels = []\n",
    "\n",
    "for label, cls in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_path, cls)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    # L·∫•y danh s√°ch file .npy trong th∆∞ m·ª•c c·ªßa ƒë·ªông t√°c hi·ªán t·∫°i\n",
    "    files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "    # X√°o tr·ªôn danh s√°ch file ƒë·ªÉ ƒë·∫£m b·∫£o train/valid ng·∫´u nhi√™n\n",
    "    np.random.shuffle(files)\n",
    "\n",
    "    # Chia theo t·ª∑ l·ªá 80% train / 20% validation\n",
    "    split_idx = int(0.8 * len(files))\n",
    "    train_videos.extend(files[:split_idx])\n",
    "    train_labels.extend([label] * split_idx)\n",
    "    valid_videos.extend(files[split_idx:])\n",
    "    valid_labels.extend([label] * (len(files) - split_idx))\n",
    "\n",
    "# T·∫°o dataset v√† dataloader cho train v√† validation\n",
    "train_dataset = YogaSkeletonDataset(train_videos, train_labels, T_max=100)\n",
    "valid_dataset = YogaSkeletonDataset(valid_videos, valid_labels, T_max=100)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Ki·ªÉm tra s·ªë l∆∞·ª£ng m·∫´u trong train/valid\n",
    "print(f\"üìå T·ªïng s·ªë m·∫´u: {len(train_dataset) + len(valid_dataset)}\")\n",
    "print(f\"‚úÖ Train set: {len(train_dataset)} samples\")\n",
    "print(f\"‚úÖ Validation set: {len(valid_dataset)} samples\")\n",
    "\n",
    "# Ki·ªÉm tra m·ªôt m·∫´u trong train set\n",
    "skeleton_sample, label_sample = train_dataset[0]\n",
    "print(\"Sample shape:\", skeleton_sample.shape)  # K·ª≥ v·ªçng: (100, 33, 3)\n",
    "print(\"Label:\", classes[label_sample.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class YogaGCN(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_dim=64, num_classes=10):\n",
    "        super(YogaGCN, self).__init__()\n",
    "        self.conv1 = gnn.GCNConv(in_channels, hidden_dim)\n",
    "        self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = torch.mean(x, dim=0)  # Pooling ƒë·ªÉ l·∫•y ƒë·∫∑c tr∆∞ng to√†n video\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# V2: th·ª≠ tƒÉng k√≠ch th∆∞·ªõc n·∫øu model v1 l·ªè    \n",
    "# class YogaGCN(nn.Module):\n",
    "#     def __init__(self, in_channels=3, hidden_dim=128, num_classes=10):\n",
    "#         super(YogaGCN, self).__init__()\n",
    "#         self.conv1 = gnn.GCNConv(in_channels, hidden_dim)\n",
    "#         self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.conv3 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.conv4 = gnn.GCNConv(hidden_dim, hidden_dim)  # Th√™m m·ªôt l·ªõp n·ªØa\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "#         x = self.conv3(x, edge_index).relu()\n",
    "#         x = self.conv4(x, edge_index).relu()\n",
    "#         x = torch.mean(x, dim=0)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# V3: th√™m dropout\n",
    "# class YogaGCN(nn.Module):\n",
    "#     def __init__(self, in_channels=3, hidden_dim=64, num_classes=10):\n",
    "#         super(YogaGCN, self).__init__()\n",
    "#         self.conv1 = gnn.GCNConv(in_channels, hidden_dim)\n",
    "#         self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.conv3 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     self.dropout = nn.Dropout(0.3)  # 30% dropout\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.conv3(x, edge_index).relu()\n",
    "#         x = torch.mean(x, dim=0)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "# V4: D√πng GAT (Graph Attention Network), ch·∫≠m h∆°n c·∫ßn GPU m·∫°nh h∆°n\n",
    "# class YogaGAT(nn.Module):\n",
    "#     def __init__(self, in_channels=3, hidden_dim=128, num_classes=10):\n",
    "#         super(YogaGAT, self).__init__()\n",
    "#         self.conv1 = gnn.GATConv(in_channels, hidden_dim, heads=4, concat=True)\n",
    "#         self.conv2 = gnn.GATConv(hidden_dim * 4, hidden_dim, heads=4, concat=True)\n",
    "#         self.conv3 = gnn.GATConv(hidden_dim * 4, hidden_dim, heads=4, concat=True)\n",
    "#         self.fc = nn.Linear(hidden_dim * 4, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         x = self.conv2(x, edge_index).relu()\n",
    "#         x = self.conv3(x, edge_index).relu()\n",
    "#         x = torch.mean(x, dim=0)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Danh s√°ch k·∫øt n·ªëi gi·ªØa c√°c node (kh·ªõp x∆∞∆°ng) d·ª±a tr√™n Mediapipe Pose\n",
    "edges = [\n",
    "    (0, 1), (1, 2), (2, 3), (0, 4), (4, 5), (5, 6), (0, 7), (0, 8), (7, 9), (8, 10),\n",
    "    (11, 12), (11, 23), (12, 24), (23, 24), (11, 13), (13, 15), (15, 17), (15, 19), (15, 21),\n",
    "    (12, 14), (14, 16), (16, 18), (16, 20), (16, 22), (23, 25), (25, 27), (27, 29), (29, 31),\n",
    "    (24, 26), (26, 28), (28, 30), (30, 32)\n",
    "]\n",
    "\n",
    "# Chuy·ªÉn th√†nh tensor cho PyTorch Geometric\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gcn(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.squeeze(1), edge_index.to(device))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return total_loss / len(train_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gcn(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs.squeeze(1), edge_index.to(device))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return total_loss / len(valid_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YogaGCN(in_channels=3, hidden_dim=64, num_classes=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üìå S·ªë l∆∞·ª£ng tham s·ªë c·ªßa YogaGCN: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # S·ªë epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_gcn(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate_gcn(model, valid_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"yoga_gcn_model.pth\")\n",
    "print(\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(model, video_path, classes, device):\n",
    "    model.eval()\n",
    "    skeleton = np.load(video_path)  # Load skeleton ƒë√£ tr√≠ch xu·∫•t\n",
    "    skeleton = torch.tensor(skeleton, dtype=torch.float32).to(device).unsqueeze(0)  # Th√™m batch dim\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(skeleton.squeeze(1), edge_index.to(device))\n",
    "        _, pred = torch.max(output, 1)\n",
    "    \n",
    "    return classes[pred.item()]\n",
    "\n",
    "# Test v·ªõi m·ªôt video\n",
    "video_path = \"skeleton_data/test/Garland_Pose/video_13.npy\"\n",
    "predicted_class = predict_video(model, video_path, classes, device)\n",
    "print(f\"‚úÖ Predicted Class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
