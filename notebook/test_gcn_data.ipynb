{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "292cd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1efe8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "from core.dataset import YogaDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49f835c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'Garland_Pose': 0, 'Happy_Baby_Pose': 1, 'Head_To_Knee_Pose': 2, 'Lunge_Pose': 3}\n",
      "Epoch 1/30 - Loss: 1.3847 - Accuracy: 22.68%\n",
      "Epoch 2/30 - Loss: 1.3686 - Accuracy: 43.30%\n",
      "Epoch 3/30 - Loss: 1.3510 - Accuracy: 51.55%\n",
      "Epoch 4/30 - Loss: 1.3122 - Accuracy: 45.36%\n",
      "Epoch 5/30 - Loss: 1.2323 - Accuracy: 62.89%\n",
      "Epoch 6/30 - Loss: 1.1451 - Accuracy: 62.89%\n",
      "Epoch 7/30 - Loss: 1.0249 - Accuracy: 62.89%\n",
      "Epoch 8/30 - Loss: 0.9020 - Accuracy: 70.10%\n",
      "Epoch 9/30 - Loss: 0.7613 - Accuracy: 81.44%\n",
      "Epoch 10/30 - Loss: 0.7253 - Accuracy: 78.35%\n",
      "Epoch 11/30 - Loss: 0.6904 - Accuracy: 71.13%\n",
      "Epoch 12/30 - Loss: 0.6374 - Accuracy: 81.44%\n",
      "Epoch 13/30 - Loss: 0.6349 - Accuracy: 79.38%\n",
      "Epoch 14/30 - Loss: 0.6104 - Accuracy: 78.35%\n",
      "Epoch 15/30 - Loss: 0.5540 - Accuracy: 80.41%\n",
      "Epoch 16/30 - Loss: 0.5277 - Accuracy: 80.41%\n",
      "Epoch 17/30 - Loss: 0.5224 - Accuracy: 82.47%\n",
      "Epoch 18/30 - Loss: 0.5803 - Accuracy: 80.41%\n",
      "Epoch 19/30 - Loss: 0.5190 - Accuracy: 84.54%\n",
      "Epoch 20/30 - Loss: 0.5523 - Accuracy: 82.47%\n",
      "Epoch 21/30 - Loss: 0.4899 - Accuracy: 82.47%\n",
      "Epoch 22/30 - Loss: 0.5111 - Accuracy: 79.38%\n",
      "Epoch 23/30 - Loss: 0.4863 - Accuracy: 81.44%\n",
      "Epoch 24/30 - Loss: 0.4638 - Accuracy: 85.57%\n",
      "Epoch 25/30 - Loss: 0.4642 - Accuracy: 81.44%\n",
      "Epoch 26/30 - Loss: 0.4552 - Accuracy: 83.51%\n",
      "Epoch 27/30 - Loss: 0.4734 - Accuracy: 80.41%\n",
      "Epoch 28/30 - Loss: 0.4690 - Accuracy: 81.44%\n",
      "Epoch 29/30 - Loss: 0.4759 - Accuracy: 82.47%\n",
      "Epoch 30/30 - Loss: 0.4625 - Accuracy: 79.38%\n"
     ]
    }
   ],
   "source": [
    "# ======================== MÔ HÌNH GCN ========================\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "class YogaGCN(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_dim=64, num_classes=4):\n",
    "        super(YogaGCN, self).__init__()\n",
    "        self.conv1 = gnn.GCNConv(in_channels, hidden_dim)\n",
    "        self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "\n",
    "        # Pooling giữ đúng batch_size\n",
    "        x = gnn.global_mean_pool(x, batch)  # (batch_size, hidden_dim)\n",
    "\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ======================== EDGE INDEX (Mediapipe) ========================\n",
    "def get_edge_index():\n",
    "    \"\"\"\n",
    "    Trả về ma trận kề (edge_index) cho 33 keypoints của Mediapipe.\n",
    "    \"\"\"\n",
    "    edges = [\n",
    "        (0, 1), (1, 2), (2, 3), (3, 7),  # Tay trái\n",
    "        (0, 4), (4, 5), (5, 6), (6, 8),  # Tay phải\n",
    "        (9, 10), (11, 12),  # Hông\n",
    "        (11, 13), (13, 15), (15, 17), (15, 19), (15, 21),  # Chân trái\n",
    "        (12, 14), (14, 16), (16, 18), (16, 20), (16, 22),  # Chân phải\n",
    "        (11, 23), (12, 24), (23, 24),  # Kết nối hông\n",
    "        (23, 25), (25, 27), (27, 29), (29, 31),  # Chân trái\n",
    "        (24, 26), (26, 28), (28, 30), (30, 32)   # Chân phải\n",
    "    ]\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # (2, num_edges)\n",
    "    return edge_index\n",
    "\n",
    "# ======================== HÀM TRAINING ========================\n",
    "def train_gcn(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    edge_index = get_edge_index().to(device)  # Edge index không thay đổi\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Giữ đúng batch size\n",
    "        batch_size, num_frames, num_keypoints, keypoint_dim = X_batch.shape\n",
    "        X_batch = X_batch.view(batch_size * num_frames * num_keypoints, keypoint_dim)  # (13200, 3)\n",
    "\n",
    "        # Tạo batch index đúng (1 batch cho mỗi video)\n",
    "        batch = torch.arange(batch_size, device=device).repeat_interleave(num_frames * num_keypoints)\n",
    "\n",
    "        # Đưa vào mô hình GCN\n",
    "        outputs = model(X_batch, edge_index, batch)  # (batch_size, num_classes)\n",
    "\n",
    "        # Kiểm tra outputs.shape\n",
    "#         print(f\"DEBUG - outputs shape: {outputs.shape}, y_batch shape: {y_batch.shape}\")\n",
    "\n",
    "        # Tính loss\n",
    "        loss = criterion(outputs, y_batch.long())  # Đảm bảo y_batch có dtype phù hợp\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return total_loss / len(train_loader), acc\n",
    "\n",
    "# ======================== TRAINING ========================\n",
    "# Dataset và DataLoader\n",
    "json_folder = \"../data/keypoints/public_data\"\n",
    "dataset = YogaDataset(json_folder, max_frames=100)\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Khởi tạo mô hình và optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YogaGCN(in_channels=3, hidden_dim=64, num_classes=4).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train thử\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    loss, acc = train_gcn(model, dataloader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f} - Accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc64eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
